{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# INFO3406 - Introduction to Data Analytics\n",
    "## Assignment 2 -  Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Instructions **\n",
    "\n",
    "#### Kaggle Display Advertising Challenge dataset will be used in this assignment. It contains of 39 features of online ads and information of if each ad was clicked or not over a period of 7 days. The semantic of these features is undisclosed. The overall objective is to determine if an ad will be clicked or not, for a set of query of features.\n",
    "\n",
    "#### For this assignment, only 100,000 ads will be used. The dataset should be downloaded according to the instructions in part 1). The first column of *\"dac_sample.txt\"*  indicates if an ad was clicked (=1) or not (=0) while rest of the 39 columns contain feature values. For this assignment you can consider all features are categorical. The values of some of these features have been hashed for anonymization purposes. Note that some features have missing values.\n",
    "\n",
    "\n",
    "\n",
    "You may view spark stages from\n",
    "http://localhost:4040/stages/ \n",
    "\n",
    "\n",
    "Some other useful resources/references:\n",
    "+ [Map-Reduce for Machine Learning on Multicore](http://papers.nips.cc/paper/3150-map-reduce-for-machine-learning-on-multicore.pdf)\n",
    "+ [Spark RDD](http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf)\n",
    "+ [Display Advertising Challenge](https://www.kaggle.com/c/criteo-display-ad-challenge)\n",
    "+ [MLlib](https://spark.apache.org/docs/1.1.0/mllib-guide.html)\n",
    "+ [MLlib: Scalable Machine Learning on Spark](http://stanford.edu/~rezab/sparkworkshop/slides/xiangrui.pdf)\n",
    "+ [Scalable Machine Learning](https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Preparing the dataset **\n",
    "\n",
    "#### Import *\"assignment2LoadData.py\"* file to the [*jupyter*  home](http://localhost:8001/tree) folder.  Run the following cell to download the dataset. After you accept the agreement, you can obtain the download URL by right-clicking on the ***\"Download Sample\"*** button and clicking \"Copy link address\" or \"Copy Link Location\", depending on your browser. Paste the URL into the # TODO in the next cell. The file is 8.4 MB compressed (.tar.gz). The script in *\"assignment2LoadData.py\"* will automatically download the file to the virtual machine (VM) and then extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0xb20d68ec>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the assignemnt 2 for INFO3406\n",
    "# SID:430028388 Hongliang Chi\n",
    "# free tp contact if there is any problem, email: hchi3573@uni.sydney.edu.au\n",
    "# [Attention]to run the code, it is needed to upload the folder called nilsimsa\n",
    "\n",
    "from IPython.lib.display import IFrame\n",
    "\n",
    "IFrame(\"http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/\",\n",
    "       800, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is already available. Nothing to do.\n"
     ]
    }
   ],
   "source": [
    "url = 'http://labs.criteo.com/wp-content/uploads/2015/04/dac_sample.tar.gz' #It may be similar to 'https://s?-eu-west-1.amazonaws.com/criteo-labs/dac.tar.gz'\n",
    "\n",
    "import assignment2LoadData as ld\n",
    "\n",
    "ld.extractData(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#extract the raw data \n",
    "import numpy as np\n",
    "import os.path\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('Assignment2', 'dac_sample.txt')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "partitions = 1\n",
    "if os.path.isfile(fileName):\n",
    "    rawData = (sc\n",
    "               .textFile(fileName, partitions)\n",
    "               .map(lambda x: x.replace('\\t', ',')))  # work with either ',' or '\\t' separated data\n",
    "    #print 'An example of rawData entry: ', rawData.take(1)\n",
    "    nData = rawData.count()\n",
    "    #print '\\nrawData count=', nData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#different weights are used in this search, like in the investigation of how speed change, I use 1000 training data \n",
    "#points and 1000 testing points, the corresponding weights = [.01,.01,0.98], the crossvalidation part is [0.95,0.05]\n",
    "#with the change of seeds. please free to change weights to run data. \n",
    "#I recommend to use small data to run the code to see the validity and reliability of code.\n",
    "\n",
    "weights = [.01,.01,.98]\n",
    "seed = 17\n",
    "\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData,rawTestData,rawValidData = rawData.randomSplit(weights, seed)\n",
    "# Cache the data, this is a application of Spark\n",
    "rawTrainData.cache()\n",
    "rawTestData.cache()\n",
    "\n",
    "#nTrain = rawTrainData.count()\n",
    "#nTest = rawTestData.count()\n",
    "#print 'rawTrainData count=', nTrain\n",
    "#print 'rawTestData count=', nTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Answers **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePoint(point):\n",
    "    #Converts a comma separated string into a list of tuples.\n",
    "    items = point.split(',')\n",
    "    return [(i, item) for i, item in enumerate(items[1:])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsedTrainFeat = rawTrainData.map(parsePoint)\n",
    "parsedTestFeat = rawTestData.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createOneHotDict(inputData):\n",
    "    #Creates a one-hot-encoder dictionary based on the input data.\n",
    "    #use Spark technique flatemap() ,distinct(),Zipwithindex90 and collect()\n",
    "    distinctFeats = inputData.flatMap(lambda x: x).distinct() \n",
    "    return distinctFeats.zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctrOHEDict = createOneHotDict(parsedTrainFeat) #Create one hot dictionary from parsedTrainFeat\n",
    "numCtrOHEFeats = len(ctrOHEDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats): \n",
    "    #Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "    sizeList = [OHEDict[f] for f in rawFeats]\n",
    "    sortedSizeList = sorted(sizeList)\n",
    "    valueList = [1 for f in rawFeats]\n",
    "    return SparseVector(numOHEFeats, sortedSizeList, valueList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parseOHEPoint(point, OHEDict, numOHEFeats): \n",
    "    #Obtain the label and feature vector for this raw observation.\n",
    "    parsedPoints = parsePoint(point) \n",
    "    items = point.split(',') \n",
    "    label = items[0] \n",
    "    features = oneHotEncoding(parsedPoints, OHEDict, numOHEFeats) \n",
    "    return LabeledPoint(label, features) \n",
    "    #return LabeledPoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[15] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OHETrainData = rawTrainData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats)) #call parseOHEPoint for each add in rawTrainData\n",
    "OHETestData = rawTestData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats))\n",
    "OHETrainData.cache()\n",
    "OHETestData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[17] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import nilsimsa\n",
    "\n",
    "# this is the body code for LSH\n",
    "# [Attention] it is needed to upload folder [Nilsimsa] to run this part of code \n",
    "# here we use LSH as a dimension-reducing technique \n",
    "\n",
    "def NilsimsaHash(numBuckets, rawFeats, printMapping=False):\n",
    "    #Calculate a feature dictionary for an observation's features based on hashing.\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        mapping[featureString] = int(int(nilsimsa.Nilsimsa(featureString).hexdigest(), 16) % numBuckets)\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "def LSHparse(point, numBuckets):\n",
    "\n",
    "    parsedPoints = parsePoint(point)\n",
    "    items = point.split(',')\n",
    "    label = items[0]\n",
    "    features = NilsimsaHash(numBuckets, parsedPoints, printMapping=False)  \n",
    "    return LabeledPoint(label, SparseVector(numBuckets, features))\n",
    "\n",
    "#we use LSH to reduce the dimension (signature) to 256\n",
    "numBucketsCTR = 2 ** 8\n",
    "\n",
    "#use MapReduce technique \n",
    "hashTrainData = rawTrainData.map(lambda x: LSHparse(x, numBucketsCTR))\n",
    "hashTrainData.cache()\n",
    "hashTestData = rawTestData.map(lambda x: LSHparse(x, numBucketsCTR))\n",
    "hashTestData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing size= 1030\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "print 'testing size=',hashTestData.count()\n",
    "#it is main body code for KNN\n",
    "\n",
    "\n",
    "def mdis(araw,braw):\n",
    "    # use manhattan distance as the distance metrics\n",
    "    a=araw.features.toArray()\n",
    "    b=braw.features.toArray()\n",
    "    distance = 0.0\n",
    "    for i in range(len(a)):\n",
    "        distance= distance + abs(a[i]-b[i])\n",
    "    print distance \n",
    "    return distance,araw.label\n",
    "\n",
    "def Knn(test,train,k):\n",
    "    #use Mapreduce and KNN to return the most likely lable from K neighbours \n",
    "    # the parameter test is only one record of testing set\n",
    "    nearest = train.map(lambda s :(mdis(s,test))).takeOrdered(k,key =lambda x:x[0]) \n",
    "    lst = [v for u,v in nearest]\n",
    "    return max(set(lst),key =lst.count)\n",
    "\n",
    "def setknn(test,train,k):\n",
    "    #run KNN on the whole testing set and output statistics and prediction \n",
    "    pre= []\n",
    "    all=test.take(test.count())\n",
    "    tp=fp=tn=fn=0.0  \n",
    "    for i in range(test.count()):\n",
    "        ins = all[i]\n",
    "        predict =  Knn(ins,train,k)\n",
    "        pre.append(predict)\n",
    "        if predict==ins.label:\n",
    "            if ins.label ==1:tp=tp+1\n",
    "            else:tn=tn+1\n",
    "        elif predict==1:fp=fp+1\n",
    "        else:fn= fn+1 \n",
    "    return tp,fp,tn,fn,pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time= start= 1446173482.37 end= 1446173916.37 total= -434.000163078\n",
      "procision= 0.318181818182 recall= 0.12389380531\n",
      "TPR= 0.12389380531 FPR 0.0746268656716\n"
     ]
    }
   ],
   "source": [
    "# code to print run time and precision and recall rate & other statistics for K=5\n",
    "# timer is used to measure time used \n",
    "# accuracy statistics is output \n",
    "# the specific prediction is pre in [tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,5)\n",
    "startk5 = time.time()\n",
    "[tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,5)\n",
    "endk5 = time.time()\n",
    "print 'time=' ,'start=',startk5,'end=',endk5,'total=',startk5 - endk5\n",
    "#print pre\n",
    "precision = tp/(tp+fp)\n",
    "recall= tp/(tp+fn)\n",
    "TPR = tp/(tp+fn)\n",
    "FPR= fp/(fp+tn)\n",
    "print 'procision=',precision,'recall=',recall\n",
    "print 'TPR=', TPR, 'FPR', FPR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# run time and precision and recall rate for K=1\\nstartk1 = time.time()\\n[tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,1)\\nendk1 = time.time()\\nprint 'time=' ,'start=',startk1,'end=',endk1,'total=',startk1 -endk1\\nprecision = tp/(tp+fp)\\nrecall= tp/(tp+fn)\\nprint 'procision=',precision,'recall=',recall\\nprint 'TPR=', TPR, 'FPR', FPR\\n\\n# run time and precision and recall rate for K=10\\nstartk10 = time.time()\\n[tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,10)\\nendk10 = time.time()\\nprint 'time=' ,'start=',startk10,'end=',endk10,'total=',startk10 -endk10\\nprecision = tp/(tp+fp)\\nrecall= tp/(tp+fn)\\nTPR = tp/(tp+fn)\\nFPR= fp/(fp+tn)\\nprint 'procision=',precision,'recall=',recall\\nprint 'TPR=', TPR, 'FPR', FPR\\n\\n# run time and precision and recall rate for K=10\\nstartk10 = time.time()\\n[tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,10)\\nendk10 = time.time()\\nprint 'time=' ,'start=',startk10,'end=',endk10,'total=',startk10 -endk10\\nprecision = tp/(tp+fp)\\nrecall= tp/(tp+fn)\\nTPR = tp/(tp+fn)\\nFPR= fp/(fp+tn)\\nprint 'procision=',precision,'recall=',recall\\nprint 'TPR=', TPR, 'FPR', FPR\\n\\n# run time and precision and recall rate for K=30\\nstartk30 = time.time()\\n[tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,30)\\nendk30 = time.time()\\nprint 'time=' ,'start=',startk30,'end=',endk30,'total=',startk30 -endk30\\nprecision = tp/(tp+fp)\\nrecall= tp/(tp+fn)\\nTPR = tp/(tp+fn)\\nFPR= fp/(fp+tn)\\nprint 'procision=',precision,'recall=',recall\\nprint 'TPR=', TPR, 'FPR', FPR\\n\\n# run time and precision and recall rate for K=100\\nstartk100 = time.time()\\n[tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,100)\\nendk100 = time.time()\\nprint 'time=' ,'start=',startk100,'end=',endk100,'total=',startk100 -endk100\\nprecision = tp/(tp+fp)\\nrecall= tp/(tp+fn)\\nprint 'procision=',precision,'recall=',recall\\nprint 'TPR=', TPR, 'FPR', FPR\\n\\n#change the cores of Spark\\n#cmd:--executor-cores 1/--executor-cores 4/--executor-cores 10\\n\\n# This is the ROC curve\\na=[0,1]\\nb=[0,1]\\nplt.plot(AFPR,ATPR)\\nplt.plot(a,b)\\nplt.show() \""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the follwing code is used for report writing\n",
    "\n",
    "\"\"\"# run time and precision and recall rate for K=1\n",
    "startk1 = time.time()\n",
    "[tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,1)\n",
    "endk1 = time.time()\n",
    "print 'time=' ,'start=',startk1,'end=',endk1,'total=',startk1 -endk1\n",
    "precision = tp/(tp+fp)\n",
    "recall= tp/(tp+fn)\n",
    "print 'procision=',precision,'recall=',recall\n",
    "print 'TPR=', TPR, 'FPR', FPR\n",
    "\n",
    "# run time and precision and recall rate for K=10\n",
    "startk10 = time.time()\n",
    "[tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,10)\n",
    "endk10 = time.time()\n",
    "print 'time=' ,'start=',startk10,'end=',endk10,'total=',startk10 -endk10\n",
    "precision = tp/(tp+fp)\n",
    "recall= tp/(tp+fn)\n",
    "TPR = tp/(tp+fn)\n",
    "FPR= fp/(fp+tn)\n",
    "print 'procision=',precision,'recall=',recall\n",
    "print 'TPR=', TPR, 'FPR', FPR\n",
    "\n",
    "# run time and precision and recall rate for K=10\n",
    "startk10 = time.time()\n",
    "[tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,10)\n",
    "endk10 = time.time()\n",
    "print 'time=' ,'start=',startk10,'end=',endk10,'total=',startk10 -endk10\n",
    "precision = tp/(tp+fp)\n",
    "recall= tp/(tp+fn)\n",
    "TPR = tp/(tp+fn)\n",
    "FPR= fp/(fp+tn)\n",
    "print 'procision=',precision,'recall=',recall\n",
    "print 'TPR=', TPR, 'FPR', FPR\n",
    "\n",
    "# run time and precision and recall rate for K=30\n",
    "startk30 = time.time()\n",
    "[tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,30)\n",
    "endk30 = time.time()\n",
    "print 'time=' ,'start=',startk30,'end=',endk30,'total=',startk30 -endk30\n",
    "precision = tp/(tp+fp)\n",
    "recall= tp/(tp+fn)\n",
    "TPR = tp/(tp+fn)\n",
    "FPR= fp/(fp+tn)\n",
    "print 'procision=',precision,'recall=',recall\n",
    "print 'TPR=', TPR, 'FPR', FPR\n",
    "\n",
    "# run time and precision and recall rate for K=100\n",
    "startk100 = time.time()\n",
    "[tp,fp,tn,fn,pre]=setknn(hashTestData,hashTrainData,100)\n",
    "endk100 = time.time()\n",
    "print 'time=' ,'start=',startk100,'end=',endk100,'total=',startk100 -endk100\n",
    "precision = tp/(tp+fp)\n",
    "recall= tp/(tp+fn)\n",
    "print 'procision=',precision,'recall=',recall\n",
    "print 'TPR=', TPR, 'FPR', FPR\n",
    "\n",
    "#change the cores of Spark\n",
    "#cmd:--executor-cores 1/--executor-cores 4/--executor-cores 10\n",
    "\n",
    "# This is the ROC curve\n",
    "a=[0,1]\n",
    "b=[0,1]\n",
    "plt.plot(AFPR,ATPR)\n",
    "plt.plot(a,b)\n",
    "plt.show() \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
